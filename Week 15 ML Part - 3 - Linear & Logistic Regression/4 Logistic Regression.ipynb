{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc74fea",
   "metadata": {},
   "source": [
    "### **Logistic Regression** - In-Depth Notes\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Introduction:**\n",
    "Logistic Regression is a statistical method commonly used for **classification tasks**. While traditionally used for **binary classification** (two classes), it can be extended for **multiclass** (more than two classes) and **ordinal classification**. Despite its name, logistic regression is not a regression algorithm but a classification algorithm. It is popular due to its simplicity, interpretability, and efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Basic Concept:**\n",
    "Logistic regression's main goal is to model the probability of a certain class. Given input features, it predicts the likelihood of an instance belonging to a specific class. The result is a probability, which is then used to classify the instance as belonging to one class or another.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Logistic Function (Sigmoid Function):**\n",
    "The core of logistic regression is the **sigmoid function**, which maps any real-valued input to a value between 0 and 1, making it interpretable as a probability.\n",
    "\n",
    "The formula for the sigmoid function is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{1 + e^{-z}} \\quad \\text{where} \\quad z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ is the predicted probability of the positive class (class = 1).\n",
    "- $z$ is the linear combination of the input features $x_1, x_2, \\dots, x_n$ weighted by the coefficients $\\beta_1, \\beta_2, \\dots, \\beta_n$, and $\\beta_0$ is the intercept.\n",
    "- The output $\\hat{y}$ lies between 0 and 1 and represents the probability of the positive class.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Interpretation of Output:**\n",
    "The output of logistic regression, $\\hat{y}$, represents the probability of an instance belonging to the positive class. Based on this probability, the model classifies the instance:\n",
    "- If $\\hat{y} \\geq 0.5$, classify the instance as the positive class (class = 1).\n",
    "- If $\\hat{y} < 0.5$, classify the instance as the negative class (class = 0).\n",
    "\n",
    "This threshold can be adjusted according to the specific problem or application.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Cost Function (Log-Loss):**\n",
    "The logistic regression model is trained by minimizing the **logarithmic loss** or **log-loss** (also known as **cross-entropy**). The objective is to find the model parameters $\\beta$ that minimize this loss function.\n",
    "\n",
    "For binary classification, the log-loss function is:\n",
    "\n",
    "$$\n",
    "J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})\\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of training examples.\n",
    "- $y^{(i)}$ is the actual label for the $i$-th example (0 or 1).\n",
    "- $\\hat{y}^{(i)}$ is the predicted probability of the $i$-th example belonging to the positive class.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Model Training:**\n",
    "Logistic regression is trained using optimization techniques such as **Gradient Descent** or **Newton's Method**. The goal is to find the parameters $\\beta$ that minimize the cost function, thereby improving the model's accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Types of Logistic Regression:**\n",
    "\n",
    "1. **Binary Logistic Regression:**\n",
    "   - Used for **binary classification** tasks where the target variable has two possible outcomes (0 or 1, True or False).\n",
    "   - **Example Use Cases**: Spam detection (Spam or Not Spam), Disease diagnosis (e.g., Cancer or No Cancer), Customer churn prediction.\n",
    "\n",
    "2. **Multinomial (Multiclass) Logistic Regression:**\n",
    "   - A generalized form of logistic regression for **multiclass classification**. The problem is decomposed into multiple binary classification tasks using the **one-vs-rest** technique.\n",
    "   - **Example Use Cases**: Handwritten digit recognition (0-9), Classifying types of flowers (Setosa, Versicolor, Virginica), Predicting customer choices from multiple options.\n",
    "\n",
    "3. **Ordinal Logistic Regression:**\n",
    "   - Used for **ordinal classification**, where the target variable has categories with a **meaningful order**, but no fixed interval between categories.\n",
    "   - **Example Use Cases**: Rating systems (Poor, Average, Good), Education levels (High School, Bachelor's, Master's), Customer satisfaction (Unsatisfied, Neutral, Satisfied).\n",
    "\n",
    "4. **Nominal Logistic Regression (Multinomial Logistic Regression for Nominal Categories):**\n",
    "   - Used when the target variable consists of **nominal categories**. Nominal categories are distinct, non-ordered classes with no inherent ranking.\n",
    "   - **Example Use Cases**: Color classification (Red, Green, Blue), Brand preference (Nike, Adidas, Puma), Geographic regions (North, South, East, West).\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Advantages of Logistic Regression:**\n",
    "1. **Simple and Interpretable**: The model is straightforward and provides interpretable results as probabilities.\n",
    "2. **Computationally Efficient**: It is a lightweight algorithm that can handle large datasets effectively.\n",
    "3. **Probabilistic Output**: The model outputs probabilities, which can be used for decision-making (e.g., determining the likelihood of an event).\n",
    "4. **Works Well for Linearly Separable Data**: Logistic regression is most effective when the data is linearly separable (or can be transformed to be separable in the log-odds space).\n",
    "5. **Flexible**: It can be applied to binary, multiclass, and ordinal classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### **9. Limitations of Logistic Regression:**\n",
    "1. **Linear Decision Boundaries**: Logistic regression assumes a linear relationship between the input variables and the log-odds of the outcome, which can be a limitation for complex, non-linear problems.\n",
    "2. **Sensitive to Outliers**: Logistic regression can be influenced by outliers, leading to overfitting.\n",
    "3. **No Complex Feature Interactions**: The model does not inherently capture interactions between features unless manually included.\n",
    "4. **Requires Feature Engineering**: In cases of non-linear relationships, feature transformations (like polynomial features) may be necessary.\n",
    "\n",
    "---\n",
    "\n",
    "#### **10. Evaluation Metrics:**\n",
    "1. **Accuracy**: The proportion of correct predictions to the total number of predictions.\n",
    "2. **Precision**: The proportion of positive predictions that are actually correct.\n",
    "3. **Recall (Sensitivity)**: The proportion of actual positives that are correctly identified by the model.\n",
    "4. **F1-Score**: The harmonic mean of precision and recall, providing a balanced measure.\n",
    "5. **Confusion Matrix**: A matrix that provides a detailed breakdown of the modelâ€™s performance, showing true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "---\n",
    "\n",
    "#### **11. Regularization in Logistic Regression:**\n",
    "Regularization techniques like **L1 (Lasso)** and **L2 (Ridge)** are used to prevent **overfitting**, especially when working with high-dimensional data. Regularization adds a penalty to the cost function to constrain the modelâ€™s coefficients.\n",
    "\n",
    "- **L1 Regularization (Lasso)**: Encourages sparsity, forcing some coefficients to zero, which can be useful for feature selection.\n",
    "- **L2 Regularization (Ridge)**: Penalizes large coefficients but does not force them to zero, which can help prevent overfitting without eliminating features.\n",
    "\n",
    "---\n",
    "\n",
    "#### **13. Comparison of Logistic Regression Types:**\n",
    "\n",
    "| **Type of Logistic Regression**      | **Target Variable**            | **Use Case**                         |\n",
    "|--------------------------------------|--------------------------------|--------------------------------------|\n",
    "| **Binary Logistic Regression**       | Two classes (0 or 1)           | Spam detection, Disease diagnosis    |\n",
    "| **Multinomial Logistic Regression**  | More than two classes          | Handwritten digit recognition, Flower classification |\n",
    "| **Ordinal Logistic Regression**      | Ordered categories (Ranked)    | Rating systems, Education levels     |\n",
    "| **Nominal Logistic Regression**      | Unordered categories           | Color classification, Brand preference |\n",
    "\n",
    "---\n",
    "\n",
    "#### **12. Practical Implementation Example:**\n",
    "\n",
    "Here is an example of implementing binary logistic regression using **Scikit-learn** in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e0949c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "data['target'] = iris.target\n",
    "\n",
    "# Split data into features and target\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1268cab9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "Here's a simplified and easy-to-understand comparison table for the different types of Logistic Regression:\n",
    "\n",
    "| **Type**                       | **Target Variable**      | **Used For**                                          | **Model Function**       | **Decision Boundaries**       | **Example Use Cases**                            |\n",
    "|---------------------------------|--------------------------|------------------------------------------------------|---------------------------|-------------------------------|--------------------------------------------------|\n",
    "| **Binary Logistic Regression**  | Binary (2 classes)       | Two-class problems (e.g., Yes/No, True/False)        | Sigmoid function          | One boundary for 2 classes    | Email spam detection, Disease diagnosis         |\n",
    "| **Multinomial Logistic Regression** | Multiclass (3+ classes) | Problems with more than two classes                  | Softmax function           | Multiple boundaries for each class | Image classification, Handwritten digit recognition |\n",
    "| **Ordinal Logistic Regression** | Ordinal (ordered categories) | Problems with ordered categories (e.g., Low, Medium, High) | Cumulative log-odds       | Ordered boundaries             | Customer satisfaction, Education levels          |\n",
    "| **Nominal Logistic Regression** | Nominal (unordered categories) | Problems with unordered categories (e.g., Red, Blue, Green) | One-vs-rest or softmax    | Multiple boundaries without order | Brand preference, Color choice                   |\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### **Logistic Regression vs. Other Algorithms**\n",
    "\n",
    "| **Feature**              | **Logistic Regression**      | **SVM (Support Vector Machines)**      | **Random Forest**         | **KNN (K-Nearest Neighbors)**   |\n",
    "|--------------------------|------------------------------|---------------------------------------|---------------------------|---------------------------------|\n",
    "| **Type of Problem**       | Classification (Binary, Multiclass) | Classification (Binary, Multiclass)    | Classification (Binary, Multiclass) | Classification (Binary, Multiclass) |\n",
    "| **Model**                 | Linear model                 | Non-linear model (kernels)            | Ensemble of decision trees | Non-parametric                  |\n",
    "| **Interpretability**      | High                         | Low (especially with non-linear kernels) | Moderate (feature importance available) | Low (black-box model)           |\n",
    "| **Training Speed**        | Fast                         | Slow (due to complex calculations)     | Moderate                  | Fast                            |\n",
    "| **Handling Non-linearity**| Only for linear decision boundaries | Handles non-linearity via kernels    | Handles non-linearity via ensemble learning | Handles non-linearity with neighbors |\n",
    "| **Overfitting Risk**      | Prone without regularization | Prone to overfitting (requires tuning) | Less prone, but requires a lot of trees | Prone to overfitting in noisy data |\n",
    "| **Best Use Case**         | Simple, linearly separable problems | Complex, non-linearly separable problems | Complex, high-dimensional data | Simple problems, less computational cost |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Conclusion:**\n",
    "Logistic regression is a simple, powerful, and interpretable classification algorithm, well-suited for binary and multi-class classification tasks. While it works best when the data is linearly separable, extensions like multinomial and ordinal logistic regression can be applied for more complex scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebdd113",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Multiclass Classification using Logistic Regression\n",
    "\n",
    "### âœ³ï¸ What is Multiclass Classification?\n",
    "Multiclass classification is a type of classification where **the output variable has more than two categories** or classes.\n",
    "\n",
    "ðŸ“Œ **Example**:  \n",
    "- Classifying animals into: `Dog`, `Cat`, `Horse`  \n",
    "- Predicting the digit from an image: `0` to `9`\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Logistic Regression: A Quick Recap\n",
    "Logistic Regression is originally used for **binary classification**:\n",
    "- Output: 0 or 1  \n",
    "- Uses **Sigmoid Function** to map predictions to probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Extending Logistic Regression to Multiclass\n",
    "\n",
    "Since logistic regression is designed for binary outputs, we need strategies to handle **multiple classes**.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Two Main Approaches\n",
    "\n",
    "### 1. **One-vs-Rest (OvR)** â€” also called **One-vs-All (OvA)**\n",
    "- For `K` classes, **K binary classifiers** are trained.\n",
    "- Each classifier separates **one class** from the **rest**.\n",
    "- At prediction time: the class with the **highest probability** is chosen.\n",
    "\n",
    "ðŸ“Œ **Example**:  \n",
    "For 3 classes: `A`, `B`, `C`, you'll train:  \n",
    "- Classifier 1: `A` vs `not A`  \n",
    "- Classifier 2: `B` vs `not B`  \n",
    "- Classifier 3: `C` vs `not C`  \n",
    "\n",
    "### 2. **Multinomial Logistic Regression (Softmax Regression)**\n",
    "- All classes are predicted **together** using a **single model**.\n",
    "- Uses **Softmax Function** instead of Sigmoid.\n",
    "- Outputs **probabilities** for each class.\n",
    "- The class with the **highest probability** is selected.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Softmax Function\n",
    "Used to **convert raw scores (logits)** into **probabilities**.\n",
    "\n",
    "ðŸ“Œ Formula for Softmax:  \n",
    "$$\n",
    "P(y = k|x) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$  \n",
    "Where:  \n",
    "- $ z_k $ is the score (linear output) for class k  \n",
    "- $ K $ is the total number of classes  \n",
    "- Output: vector of probabilities that sum to 1\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Cross-Entropy Loss (used in Multiclass)\n",
    "The loss function used is:\n",
    "$$\n",
    "\\text{Loss} = - \\sum_{k=1}^{K} y_k \\log(P(y=k|x))\n",
    "$$\n",
    "Where:  \n",
    "- $ y_k $ is 1 if the class is correct, 0 otherwise  \n",
    "- $ P(y=k|x) $ is the predicted probability for class k\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Model Training\n",
    "\n",
    "### 1. **Hypothesis Function** (for class *k*):\n",
    "$$\n",
    "z_k = W_k^T x + b_k\n",
    "$$\n",
    "Then apply Softmax on all $ z_k $ to get class probabilities.\n",
    "\n",
    "### 2. **Loss Function**:\n",
    "Use **categorical cross-entropy** to measure error.\n",
    "\n",
    "### 3. **Optimization**:\n",
    "Gradient Descent or other optimizers are used to minimize loss.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Example (Intuition)\n",
    "\n",
    "Let's say we want to classify fruit images into:\n",
    "- ðŸŽ Apple  \n",
    "- ðŸŒ Banana  \n",
    "- ðŸ‡ Grapes\n",
    "\n",
    "For each image `x`:\n",
    "- The model outputs a score vector (logits), e.g., [2.0, 1.0, 0.1]  \n",
    "- Apply softmax â†’ [0.64, 0.24, 0.12]  \n",
    "- Class with max probability (Apple, 0.64) is predicted.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Multiclass Logistic Regression in Python (with Scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac25c641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "Actual classes   : [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "Accuracy         : 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=200)  # Removed multi_class to avoid warning\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Output\n",
    "print(\"Predicted classes:\", y_pred)\n",
    "print(\"Actual classes   :\", y_test)\n",
    "print(\"Accuracy         :\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a30fa1",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ”¶ When to Use Which?\n",
    "\n",
    "| Approach        | When to Use                                      |\n",
    "|----------------|--------------------------------------------------|\n",
    "| One-vs-Rest     | Small number of classes, easier to interpret     |\n",
    "| Multinomial     | Large number of classes, better overall accuracy |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Evaluation Metrics\n",
    "\n",
    "- **Accuracy**  \n",
    "- **Precision, Recall, F1-Score (per class)**  \n",
    "- **Confusion Matrix**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Advantages of Logistic Regression for Multiclass\n",
    "\n",
    "- Simple and interpretable\n",
    "- Fast training for small to medium datasets\n",
    "- Works well when the relationship is linear\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Limitations\n",
    "\n",
    "- Not great with complex patterns (non-linear boundaries)\n",
    "- Performance may degrade with large feature sets or overlapping classes\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
