{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657d10bb",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è **Hyperparameter Tuning in Machine Learning**\n",
    "\n",
    "---\n",
    "\n",
    "## üìå **1. What are Hyperparameters?**\n",
    "\n",
    "- **Definition**:  \n",
    "  Hyperparameters are **external configurations** to the model which **cannot be learned from the data**. Instead, they are set **before** the training process begins and govern the learning process itself.\n",
    "\n",
    "- **Examples**:\n",
    "  - `n_estimators` (Number of trees) in Random Forest\n",
    "  - `max_depth` (Maximum tree depth)\n",
    "  - `learning_rate` in Gradient Boosting\n",
    "  - `C` and `gamma` in SVM\n",
    "  - `k` in KNN\n",
    "\n",
    "- **Hyperparameter vs Parameter**:\n",
    "\n",
    "| **Aspect**      | **Hyperparameter**                                    | **Parameter**                                         |\n",
    "|-----------------|--------------------------------------------------------|--------------------------------------------------------|\n",
    "| Definition      | Set before training, controls the learning process     | Learned from data during training                      |\n",
    "| Example         | `learning_rate`, `n_estimators`                        | `weights` and `biases` in linear models               |\n",
    "| Tuning Method   | GridSearch, RandomizedSearch, Bayesian Optimization    | Backpropagation, Gradient Descent                     |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **2. Why is Hyperparameter Tuning Important?**\n",
    "\n",
    "- To **maximize model accuracy** and **reduce errors**\n",
    "- To prevent **overfitting** or **underfitting**\n",
    "- To achieve **optimal generalization** on unseen data\n",
    "- To build **efficient and fast models**\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ **3. Methods of Hyperparameter Tuning**\n",
    "\n",
    "---\n",
    "\n",
    "### üü© **A. Grid Search Cross-Validation (`GridSearchCV`)**\n",
    "\n",
    "- **How it works**:\n",
    "  - Try **all possible combinations** of hyperparameters.\n",
    "  - Use **cross-validation** to evaluate each combination.\n",
    "\n",
    "- **Pros**:\n",
    "  - Simple and exhaustive\n",
    "  - Guarantees to find the best combo (within the grid)\n",
    "\n",
    "- **Cons**:\n",
    "  - Very **time-consuming**, especially for large parameter grids\n",
    "\n",
    "- **Python Example**:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, None]\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üü® **B. Randomized Search Cross-Validation (`RandomizedSearchCV`)**\n",
    "\n",
    "- **How it works**:\n",
    "  - Randomly selects a **fixed number of hyperparameter combinations** from a grid or distribution.\n",
    "  - Uses cross-validation for evaluation.\n",
    "\n",
    "- **Pros**:\n",
    "  - Much **faster** than GridSearchCV\n",
    "  - Can explore a **larger space** with limited resources\n",
    "\n",
    "- **Cons**:\n",
    "  - Might **miss the best** combination\n",
    "\n",
    "- **Python Example**:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': [5, 10, None]\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Accuracy:\", random_search.best_score_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üü¶ **C. Manual Search**\n",
    "\n",
    "- You try combinations of parameters manually based on **experience or intuition**.\n",
    "- Less efficient but good for **quick prototyping or small models**.\n",
    "\n",
    "---\n",
    "\n",
    "### üüß **D. Bayesian Optimization / Advanced Search (e.g., Optuna, Hyperopt, Skopt)**\n",
    "\n",
    "- Uses **probabilistic models** to decide which combinations to try next.\n",
    "- Learns from **previous trials** to suggest **better hyperparameters**.\n",
    "- **More efficient** than Grid/Random Search for large search spaces.\n",
    "\n",
    "> **Tools**:  \n",
    "‚úÖ `Optuna`  \n",
    "‚úÖ `Hyperopt`  \n",
    "‚úÖ `BayesianSearchCV` (from `scikit-optimize`)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è **4. Hyperparameter Tuning Workflow**\n",
    "\n",
    "```text\n",
    "1. Choose the algorithm ‚Üí \n",
    "2. Identify the hyperparameters to tune ‚Üí\n",
    "3. Define the range of values ‚Üí\n",
    "4. Select tuning method (Grid, Random, etc.) ‚Üí\n",
    "5. Use cross-validation to evaluate ‚Üí\n",
    "6. Choose best parameters ‚Üí\n",
    "7. Train final model on full training data ‚Üí\n",
    "8. Evaluate on test set\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìà **5. Common Hyperparameters by Model**\n",
    "\n",
    "| **Algorithm**           | **Key Hyperparameters**                                                |\n",
    "|--------------------------|------------------------------------------------------------------------|\n",
    "| **Logistic Regression**  | `C`, `penalty`, `solver`                                               |\n",
    "| **Decision Tree**        | `max_depth`, `min_samples_split`, `criterion`                          |\n",
    "| **Random Forest**        | `n_estimators`, `max_features`, `max_depth`, `bootstrap`               |\n",
    "| **Gradient Boosting**    | `learning_rate`, `n_estimators`, `subsample`, `max_depth`              |\n",
    "| **KNN**                  | `n_neighbors`, `weights`, `algorithm`                                  |\n",
    "| **SVM**                  | `C`, `kernel`, `gamma`                                                 |\n",
    "| **Neural Networks**      | `learning_rate`, `batch_size`, `activation`, `hidden_layer_sizes`, etc |\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **6. Visualizing Tuning Results (Optional but Helpful)**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "mean_scores = results['mean_test_score']\n",
    "params = [str(p) for p in results['params']]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mean_scores)\n",
    "plt.xticks(ticks=range(len(params)), labels=params, rotation=90)\n",
    "plt.xlabel('Hyperparameter Combinations')\n",
    "plt.ylabel('Mean Test Score')\n",
    "plt.title('Grid Search Results')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **7. Tips for Effective Hyperparameter Tuning**\n",
    "\n",
    "- Use **RandomizedSearch** for large spaces, **GridSearch** for small ones.\n",
    "- Always **shuffle and stratify** (if classification).\n",
    "- Normalize/scale data before tuning SVM/KNN/Logistic.\n",
    "- Start with **default values**, then tune 1-2 at a time.\n",
    "- Use **early stopping** in boosting/NNs to prevent overfitting.\n",
    "- Combine with **cross-validation** for robust results.\n",
    "- Track results using **MLflow**, **TensorBoard**, or logs.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ **8. Hyperparameter Tuning vs Model Selection**\n",
    "\n",
    "| **Aspect**           | **Hyperparameter Tuning**           | **Model Selection**                         |\n",
    "|----------------------|--------------------------------------|---------------------------------------------|\n",
    "| Goal                 | Optimize one model's settings        | Choose the best algorithm                   |\n",
    "| Involves             | `GridSearchCV`, `RandomizedSearchCV`| Comparing multiple algorithms                |\n",
    "| Example              | Best `max_depth` in Decision Tree    | Decision Tree vs SVM vs KNN                 |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### ‚öñÔ∏è **GridSearchCV vs RandomizedSearchCV**\n",
    "\n",
    "| Feature                 | GridSearchCV                           | RandomizedSearchCV                        |\n",
    "|------------------------|----------------------------------------|-------------------------------------------|\n",
    "| Search Method          | Exhaustive (tries all combinations)    | Random sampling from parameter space      |\n",
    "| Speed                  | Slower (especially with many options)  | Faster                                    |\n",
    "| Best For               | Small search spaces                    | Large or unknown search spaces            |\n",
    "| Precision              | More precise                           | Approximate (but usually good enough)     |\n",
    "| Control                | You control all combinations           | You control number of iterations (`n_iter`) |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Python Code Example for RandomizedSearchCV**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ed029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 8, 'n_estimators': 190}\n",
      "Best Score: 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Load data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Define model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define parameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(3, 15)\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, \n",
    "                                   n_iter=10, cv=5, scoring='accuracy', \n",
    "                                   random_state=1)\n",
    "\n",
    "# Fit model\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
