{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b11bcd",
   "metadata": {},
   "source": [
    "# **Classification in Machine Learning** Detailed Notes \n",
    "\n",
    "---\n",
    "\n",
    "### **What is Classification?**\n",
    "\n",
    "**Classification** is a supervised learning technique where the model learns from labeled training data to predict the category or class label of new data points. It is used when the output variable is categorical in nature. The objective of classification is to predict the discrete class labels.\n",
    "\n",
    "For instance, the task could be to classify emails as either \"Spam\" or \"Not Spam,\" or to classify an image of a fruit as \"Apple,\" \"Banana,\" or \"Orange.\"\n",
    "\n",
    "---\n",
    "\n",
    "## **Classification in Machine Learning - Key Concepts**\n",
    "\n",
    "### **Supervised Learning**\n",
    "In classification, we use **supervised learning** where the model is trained on a labeled dataset, meaning that each training example is paired with a label. The model learns to map input features to their respective class labels.\n",
    "\n",
    "### **Classes and Labels**\n",
    "The output or **label** is a discrete value, meaning the possible outcomes are **finite and predefined**. For example:\n",
    "- **Binary Classification**: Two classes (e.g., \"Yes\" or \"No\").\n",
    "- **Multi-Class Classification**: More than two classes (e.g., \"Cat,\" \"Dog,\" \"Bird\").\n",
    "- **Multi-Label Classification**: An instance can belong to multiple classes simultaneously (e.g., tags in a blog post).\n",
    "\n",
    "### **Training a Classification Model**\n",
    "In training a classification model:\n",
    "1. The model learns the relationship between **input features** and **class labels** in the training data.\n",
    "2. Once trained, the model can predict the class labels for **unseen data**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Classification Algorithms**\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Logistic Regression (Binary Classification)**\n",
    "\n",
    "**Logistic Regression** is a statistical method used for binary classification. Despite its name, it is a **classification algorithm**, not a regression one.\n",
    "\n",
    "- **How it works**: Logistic regression uses the **logistic (sigmoid)** function to model the probability that a data point belongs to one of the two classes.\n",
    "\n",
    "- **Sigmoid Function**:\n",
    "  $$\n",
    "  \\hat{y} = \\frac{1}{1 + e^{-z}}, \\quad z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\n",
    "  $$\n",
    "  where $ z $ is a linear combination of the input features $ x_1, x_2, \\dots, x_n $.\n",
    "\n",
    "- **Outcome**: The output of logistic regression is a probability (between 0 and 1). A threshold (usually 0.5) is used to assign a class label (e.g., if the output is >0.5, classify as \"1\" or \"True\").\n",
    "\n",
    "- **Use Cases**: \n",
    "  - Email spam classification.\n",
    "  - Medical diagnosis (e.g., predicting whether a tumor is malignant or benign).\n",
    "\n",
    "- **Advantages**:\n",
    "  - Simple and easy to implement.\n",
    "  - Interpretability: The coefficients indicate the importance of each feature.\n",
    "  - Works well for linearly separable data.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Assumes a linear decision boundary, which may not work well for complex data.\n",
    "  - Sensitive to feature scaling (may need normalization).\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. K-Nearest Neighbors (KNN)**\n",
    "\n",
    "**K-Nearest Neighbors (KNN)** is a simple, instance-based learning algorithm. It does not make assumptions about the data distribution and works by finding the most similar data points to a new data point.\n",
    "\n",
    "- **How it works**: Given a data point, KNN looks at the **K nearest neighbors** (using a distance metric like Euclidean distance) and assigns the class label based on the majority vote among those neighbors.\n",
    "\n",
    "- **Use Case**:\n",
    "  - Image classification.\n",
    "  - Recommender systems.\n",
    "  - Handwritten digit recognition.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Simple and intuitive.\n",
    "  - Non-parametric (no assumptions about the data).\n",
    "  - Performs well for small to medium datasets.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Computationally expensive for large datasets (high prediction time).\n",
    "  - Sensitive to irrelevant or redundant features.\n",
    "  - Performance depends heavily on the choice of $ K $ and distance metric.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Decision Tree Classifier**\n",
    "\n",
    "**Decision Trees** create a model based on a series of decisions. Each internal node of the tree represents a decision on a feature, and each leaf node represents a class label.\n",
    "\n",
    "- **How it works**: A decision tree splits the data based on the feature that provides the **best split** (using metrics like Gini Impurity or Entropy).\n",
    "\n",
    "- **Use Case**:\n",
    "  - Customer segmentation.\n",
    "  - Predicting loan approval.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Easy to interpret and visualize.\n",
    "  - Handles both numerical and categorical data.\n",
    "  - Can handle missing data.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Prone to overfitting, especially with deep trees.\n",
    "  - Unstable, small changes in the data can cause large changes in the tree structure.\n",
    "  - Often biased towards features with more levels.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Random Forest Classifier**\n",
    "\n",
    "**Random Forest** is an ensemble learning method that uses multiple decision trees to improve classification accuracy. It uses **bagging** (Bootstrap Aggregating) to build several trees and combines their outputs.\n",
    "\n",
    "- **How it works**: It trains multiple decision trees on bootstrapped data and uses majority voting for classification.\n",
    "\n",
    "- **Use Case**:\n",
    "  - Stock market prediction.\n",
    "  - Fraud detection.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Reduces overfitting compared to a single decision tree.\n",
    "  - Can handle high-dimensional datasets.\n",
    "  - Robust to outliers.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Less interpretable than individual decision trees.\n",
    "  - Computationally expensive (especially for large datasets).\n",
    "  - Can overfit if not tuned properly.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Support Vector Machine (SVM)**\n",
    "\n",
    "**Support Vector Machine (SVM)** aims to find the **optimal hyperplane** that separates the data into different classes, maximizing the margin between the classes.\n",
    "\n",
    "- **How it works**: SVM tries to find the hyperplane that best separates data points from different classes. It can be used for both **linear** and **non-linear** data, with the help of **kernel functions**.\n",
    "\n",
    "- **Use Case**:\n",
    "  - Text classification (e.g., spam detection).\n",
    "  - Image classification (e.g., facial recognition).\n",
    "\n",
    "- **Advantages**:\n",
    "  - Effective in high-dimensional spaces.\n",
    "  - Works well when there is a clear margin of separation between classes.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Does not work well with large datasets (training time can be slow).\n",
    "  - Difficult to tune the parameters (e.g., $ C $, kernel function).\n",
    "  - Sensitive to the choice of kernel.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Naive Bayes Classifier**\n",
    "\n",
    "**Naive Bayes** is a probabilistic classifier based on **Bayes’ Theorem**, assuming that the features are **independent** (which is often not true).\n",
    "\n",
    "- **How it works**: The algorithm calculates the probability of each class given the features and assigns the class with the highest probability.\n",
    "\n",
    "- **Use Case**:\n",
    "  - Document classification (e.g., spam detection).\n",
    "  - Sentiment analysis.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Simple and fast.\n",
    "  - Works well with high-dimensional data, especially text data.\n",
    "  - Performs well even when the assumption of independence is violated.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Assumes that features are conditionally independent, which is often unrealistic.\n",
    "  - Does not perform well with highly correlated features.\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Gradient Boosting Classifier (e.g., XGBoost, LightGBM)**\n",
    "\n",
    "**Gradient Boosting** is an ensemble learning technique that builds a model sequentially, with each new model correcting the errors of the previous ones using gradient descent.\n",
    "\n",
    "- **How it works**: It builds an ensemble of weak learners (typically decision trees) where each tree focuses on the errors made by the previous tree.\n",
    "\n",
    "- **Use Case**:\n",
    "  - Fraud detection.\n",
    "  - Classification problems with complex relationships.\n",
    "\n",
    "- **Advantages**:\n",
    "  - High accuracy and performance.\n",
    "  - Can handle missing values and different types of data.\n",
    "  - Works well on complex datasets.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Computationally expensive.\n",
    "  - Sensitive to overfitting if not tuned properly.\n",
    "  - Can take a long time to train.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Neural Networks (Multi-Layer Perceptron - MLP)**\n",
    "\n",
    "**Neural Networks** are inspired by the human brain, consisting of layers of neurons that process and classify data through activation functions.\n",
    "\n",
    "- **How it works**: The network learns through layers of weighted connections. Each layer's output is passed as input to the next layer until the final output layer generates the prediction.\n",
    "\n",
    "- **Use Case**:\n",
    "  - Image classification (e.g., CNN for visual tasks).\n",
    "  - Speech recognition.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Can model complex relationships and decision boundaries.\n",
    "  - Works well for large datasets.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Difficult to interpret (\"black-box\" model).\n",
    "  - Requires large datasets and computational resources.\n",
    "  - Prone to overfitting without proper regularization.\n",
    "\n",
    "---\n",
    "\n",
    "## **Classification Evaluation Metrics**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Accuracy**\n",
    "\n",
    "- **Definition**: The percentage of correctly classified instances.\n",
    "\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "  $$\n",
    "\n",
    "- **Limitations**: Not reliable for imbalanced datasets (where one class significantly outnumbers the other).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Precision**\n",
    "\n",
    "- **Definition**: The proportion of positive predictions that are actually correct.\n",
    "\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "  $$\n",
    "\n",
    "- **Use Case**: Important when the cost of false positives is high (e.g., medical diagnoses).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Recall (Sensitivity)**\n",
    "\n",
    "- **Definition**: The proportion of actual positives that are correctly identified.\n",
    "\n",
    "  $$\n",
    "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "  $$\n",
    "\n",
    "- **Use Case**: Important when the cost of false negatives is high (e.g., in fraud detection).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. F1 Score**\n",
    "\n",
    "- **Definition**: The harmonic mean of precision and recall, useful when both metrics need to be balanced.\n",
    "\n",
    "  $$\n",
    "  F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Confusion Matrix**\n",
    "\n",
    "A **confusion matrix** shows the breakdown of predictions, including:\n",
    "\n",
    "- **TP (True Positive)**: Correctly predicted positive cases.\n",
    "- **TN (True Negative)**: Correctly predicted negative cases.\n",
    "- **FP (False Positive)**: Incorrectly predicted as positive.\n",
    "- **FN (False Negative)**: Incorrectly predicted as negative.\n",
    "\n",
    "\n",
    "Here’s a **Confusion Matrix** table that you can use to evaluate classification models:\n",
    "\n",
    "|                | **Predicted Positive** | **Predicted Negative** |\n",
    "|----------------|------------------------|------------------------|\n",
    "| **Actual Positive** | True Positive (TP)      | False Negative (FN)     |\n",
    "| **Actual Negative** | False Positive (FP)     | True Negative (TN)      |\n",
    "\n",
    "### Explanation of Terms:\n",
    "- **True Positive (TP)**: The number of instances where the model correctly predicted the positive class.\n",
    "- **True Negative (TN)**: The number of instances where the model correctly predicted the negative class.\n",
    "- **False Positive (FP)**: The number of instances where the model incorrectly predicted the positive class (Type I error).\n",
    "- **False Negative (FN)**: The number of instances where the model incorrectly predicted the negative class (Type II error).\n",
    "\n",
    "This matrix helps you assess the performance of a classification model and can be used to compute metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. ROC Curve and AUC**\n",
    "\n",
    "- **ROC Curve (Receiver Operating Characteristic Curve)**: A plot of **True Positive Rate (Recall)** vs. **False Positive Rate**.\n",
    "- **AUC (Area Under the Curve)**: The area under the ROC curve. A higher AUC means a better model.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "Classification is a critical concept in machine learning that applies to various real-world problems. Different algorithms offer various trade-offs in terms of complexity, accuracy, and interpretability. Choosing the right algorithm depends on the dataset, problem type, and evaluation metrics.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
