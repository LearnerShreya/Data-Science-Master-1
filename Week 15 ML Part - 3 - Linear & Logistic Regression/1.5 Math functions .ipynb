{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1054930b",
   "metadata": {},
   "source": [
    "### Common Mathematical Functions in Machine Learning\n",
    "\n",
    "Mathematical functions are essential in machine learning, especially in neural networks and deep learning, for modeling non-linearity, activation, optimization, and distance measures. Below is a detailed explanation of the most commonly used functions in machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Linear Function**\n",
    "\n",
    "- **Mathematical Formula**:  \n",
    "  $$\n",
    "  f(x) = ax + b\n",
    "  $$\n",
    "- **Range**: $(-\\infty, \\infty)$\n",
    "- **Purpose**: A linear function describes a straight-line relationship between the input $x$ and the output $f(x)$. It is used to model relationships where the dependent variable changes linearly with the independent variable.\n",
    "- **Use Cases**:  \n",
    "  - **Linear Regression**: To model the relationship between a dependent and independent variable with a straight-line fit.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Sigmoid Function (Logistic Function)**\n",
    "\n",
    "- **Mathematical Formula**:  \n",
    "  $$\n",
    "  S(x) = \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "- **Range**: $(0, 1)$\n",
    "- **Purpose**: The sigmoid function outputs a value between 0 and 1. It is widely used in classification problems to model probabilities.\n",
    "- **Use Cases**:  \n",
    "  - **Binary Classification**: In algorithms like **Logistic Regression** to predict the probability of a class.\n",
    "  - **Neural Networks**: As an activation function in the output layer for binary classification tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Tangent Hyperbolic (tanh) Function**\n",
    "\n",
    "- **Mathematical Formula**:  \n",
    "  $$\n",
    "  \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "  $$\n",
    "- **Range**: $(-1, 1)$\n",
    "- **Purpose**: The **tanh** function is a scaled version of the sigmoid function, providing output values between -1 and 1. It helps in centering the data around zero, making training of models more efficient.\n",
    "- **Use Cases**:  \n",
    "  - **Hidden Layers in Neural Networks**: Often used in deep networks for faster convergence and reducing gradient issues.\n",
    "  - **Non-linear transformation** of data for better representation.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Rectified Linear Unit (ReLU)**\n",
    "\n",
    "- **Mathematical Formula**:  \n",
    "  $$\n",
    "  f(x) = \\max(0, x)\n",
    "  $$\n",
    "- **Range**: $[0, \\infty)$\n",
    "- **Purpose**: **ReLU** outputs the input directly if it is positive; otherwise, it outputs zero. This makes it computationally efficient and helps avoid vanishing gradient problems during backpropagation in deep neural networks.\n",
    "- **Use Cases**:  \n",
    "  - **Hidden Layers in Neural Networks**: ReLU is the default activation function for many deep learning models due to its efficiency and simplicity.\n",
    "  - **Convolutional Neural Networks (CNNs)** for feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Leaky ReLU**\n",
    "\n",
    "- **Mathematical Formula**:  \n",
    "  $$\n",
    "  f(x) = \\begin{cases} \n",
    "  x & \\text{if } x > 0 \\\\\n",
    "  \\alpha x & \\text{if } x \\leq 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "  where $\\alpha$ is a small constant (e.g., 0.01).\n",
    "- **Range**: $(-\\infty, \\infty)$\n",
    "- **Purpose**: Leaky ReLU allows a small, non-zero output for negative inputs, preventing the \"dying ReLU\" problem, where neurons stop learning if they output zero for all inputs.\n",
    "- **Use Cases**:  \n",
    "  - **Hidden Layers in Neural Networks**: Especially useful in deep networks where ReLU might cause certain neurons to die during training.\n",
    "  \n",
    "---\n",
    "\n",
    "### 6. **Softmax Function**\n",
    "\n",
    "- **Mathematical Formula**:  \n",
    "  $$\n",
    "  \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "  $$\n",
    "- **Range**: $(0, 1)$ for each output, and the outputs sum to 1.\n",
    "- **Purpose**: Softmax is used in **multi-class classification** to convert raw outputs (logits) from a model into a probability distribution. It ensures that the sum of the probabilities of all classes equals 1.\n",
    "- **Use Cases**:  \n",
    "  - **Output Layer in Neural Networks** for multi-class classification tasks.\n",
    "  - **Categorical Cross-Entropy Loss**: In classification tasks to measure the performance of the model.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Softplus Function**\n",
    "\n",
    "- **Mathematical Formula**:  \n",
    "  $$\n",
    "  f(x) = \\ln(1 + e^x)\n",
    "  $$\n",
    "- **Range**: $(0, \\infty)$\n",
    "- **Purpose**: The **Softplus** function is a smooth approximation of the ReLU function, addressing its sharp transition by having a smooth gradient, especially useful for optimization in deep networks.\n",
    "- **Use Cases**:  \n",
    "  - **Activation Function** in deep networks when a smooth version of ReLU is needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Swish Function**\n",
    "\n",
    "- **Mathematical Formula**:  \n",
    "  $$\n",
    "  f(x) = x \\cdot \\sigma(x)\n",
    "  $$\n",
    "  where $\\sigma(x)$ is the sigmoid function.\n",
    "- **Range**: $(-\\infty, \\infty)$\n",
    "- **Purpose**: The **Swish** function is a newer activation function that combines the advantages of ReLU and sigmoid. It has been shown to outperform ReLU in deeper networks, offering smoother gradients.\n",
    "- **Use Cases**:  \n",
    "  - **Hidden Layers in Neural Networks** for improved model performance.\n",
    "  \n",
    "---\n",
    "\n",
    "### 9. **Exponential Linear Unit (ELU)**\n",
    "\n",
    "- **Mathematical Formula**:  \n",
    "  $$\n",
    "  f(x) = \\begin{cases} \n",
    "  x & \\text{if } x > 0 \\\\\n",
    "  \\alpha (e^x - 1) & \\text{if } x \\leq 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "  where $\\alpha$ is a constant, typically 1.\n",
    "- **Range**: $(-\\infty, \\infty)$\n",
    "- **Purpose**: ELU is similar to ReLU but with an exponential function for negative values, which helps avoid the dying neuron problem and can lead to faster convergence.\n",
    "- **Use Cases**:  \n",
    "  - **Hidden Layers in Neural Networks** for deep learning architectures where ReLU might cause saturation in gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Gaussian Function**\n",
    "\n",
    "- **Mathematical Formula**:  \n",
    "  $$\n",
    "  f(x) = e^{-\\frac{x^2}{2\\sigma^2}}\n",
    "  $$\n",
    "- **Range**: $(0, 1]$\n",
    "- **Purpose**: The **Gaussian function** is commonly used as a kernel in machine learning algorithms and for smoothing. It is used in probability distributions and kernel methods.\n",
    "- **Use Cases**:  \n",
    "  - **Gaussian Naive Bayes**: In classification tasks when features follow a Gaussian distribution.\n",
    "  - **Kernel Functions** in **Support Vector Machines (SVM)** for non-linear classification.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. **Hard Sigmoid**\n",
    "\n",
    "- **Mathematical Formula**:  \n",
    "  $$\n",
    "  f(x) = \\min(\\max(0, 0.2x + 0.5), 1)\n",
    "  $$\n",
    "- **Range**: $(0, 1)$\n",
    "- **Purpose**: The **Hard Sigmoid** function is a computationally efficient approximation of the sigmoid function, providing a simpler calculation.\n",
    "- **Use Cases**:  \n",
    "  - **Efficient Neural Networks** where computational speed is crucial, and exact sigmoid function is not necessary.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. **Cosine Function**\n",
    "\n",
    "- **Mathematical Formula**:  \n",
    "  $$\n",
    "  f(x) = \\cos(x)\n",
    "  $$\n",
    "- **Range**: $(-1, 1)$\n",
    "- **Purpose**: The **Cosine** function is used in **distance measures** to calculate similarity between vectors.\n",
    "- **Use Cases**:  \n",
    "  - **Cosine Similarity**: In text mining and Natural Language Processing (NLP) to measure the similarity between two vectors (e.g., document vectors).\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Common Functions in Machine Learning\n",
    "\n",
    "- **Linear Functions**: Used in simple regression tasks to model linear relationships.\n",
    "- **Activation Functions**: Sigmoid, Tanh, ReLU, Leaky ReLU, Softmax, Softplus, Swish, and ELU are non-linear functions used in neural networks to introduce non-linearity and enable better learning of complex data representations.\n",
    "- **Distance and Similarity Functions**: Cosine function is commonly used in similarity metrics such as cosine similarity for vector comparison.\n",
    "- **Smooth Activation Functions**: Functions like Softplus, Swish, and ELU provide smooth transitions, helping with optimization in deep learning.\n",
    "\n",
    "These functions play vital roles in different machine learning models, helping them learn from data more effectively and model complex patterns in the data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
