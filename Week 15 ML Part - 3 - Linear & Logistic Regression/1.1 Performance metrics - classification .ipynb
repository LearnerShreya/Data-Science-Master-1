{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eebe516",
   "metadata": {},
   "source": [
    "# ðŸ“Š Performance Metrics in Classification Model\n",
    "\n",
    "Performance metrics help assess how well a machine learning model performs. The choice of metric depends on the **problem type**â€”whether it's classification, regression, or clustering. \n",
    "\n",
    "## ðŸ”· **Why is Evaluation Important?** \n",
    "\n",
    "After building a model, you need to know:\n",
    "- Is it accurate?\n",
    "- Is it reliable?\n",
    "- Is it overfitting or underfitting?\n",
    "- How well does it generalize to unseen data?\n",
    "\n",
    "ðŸ‘‰ Thatâ€™s where evaluation metrics come into play.\n",
    "\n",
    "# ðŸ”· Classification Metrics  \n",
    "> Used to evaluate the performance of **classification models**, where output is **categorical** (e.g., Spam/Not Spam, Disease/No Disease, Yes/No, etc.).\n",
    "\n",
    "### ðŸ”¸ **Confusion Matrix**\n",
    "|                        | **Predicted Positive** | **Predicted Negative** |\n",
    "|------------------------|------------------------|------------------------|\n",
    "| **Actual Positive**    | True Positive (TP)     | False Negative (FN)   |\n",
    "| **Actual Negative**    | False Positive (FP)    | True Negative (TN)    |\n",
    "\n",
    "\n",
    "Helps visualize:\n",
    "- Types of errors\n",
    "- Metric computation\n",
    "\n",
    "\n",
    "## ðŸ”¸ **Comprehensive Classification Metrics Table**\n",
    "\n",
    "| Metric               | Best Use Case                            | Range       | Formula Complexity | Formula                                                                                  | Advantages                                                  | Disadvantages                                               | Additional Use Cases                                        |\n",
    "|----------------------|------------------------------------------|-------------|---------------------|------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Accuracy**         | Balanced datasets                        | 0 to 1      | Low                 | $ \\frac{TP + TN}{TP + TN + FP + FN} $                                                   | Easy to understand and compute                              | Misleading when data is imbalanced                         | Balanced datasets                                           |\n",
    "| **Precision**        | Minimise False Positives                 | 0 to 1      | Low                 | $ \\frac{TP}{TP + FP} $                                                                  | Reduces false positives, useful when false positives are costly | Ignores false negatives                                     | Spam detection, fraud detection                             |\n",
    "| **Recall** (Sensitivity) | Minimise False Negatives             | 0 to 1      | Low                 | $ \\frac{TP}{TP + FN} $                                                                  | Captures all actual positives                               | Ignores false positives                                     | Medical diagnosis, search relevance                         |\n",
    "| **Specificity** (TNR) | Focus on identifying actual negatives   | 0 to 1      | Low                 | $ \\frac{TN}{TN + FP} $                                                                 | Focuses on correct prediction of actual negatives           | Doesnâ€™t consider true positives                            | Crime detection, background screening                       |\n",
    "| **F1-Score**         | Balance of Precision & Recall            | 0 to 1      | Medium              | $ 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $                        | Balances precision and recall                               | Harder to interpret intuitively                             | Imbalanced classification tasks                             |\n",
    "| **FÎ²-Score**         | Preference-weighted scenarios            | 0 to 1      | Medium              | $ (1 + \\beta^2) \\cdot \\frac{Precision \\cdot Recall}{\\beta^2 \\cdot Precision + Recall} $ | Adjusts weight between precision and recall                 | Needs tuning of Î²                                           | Î² > 1 for recall focus (medical), Î² < 1 for precision focus |\n",
    "| **Balanced Accuracy** | Imbalanced datasets                     | 0 to 1      | Medium              | $ \\frac{Recall + Specificity}{2} $                                                     | Handles imbalanced datasets better than regular accuracy    | Less popular, can be misunderstood                         | Imbalanced datasets with both FN and FP concern             |\n",
    "| **MCC**              | Highly imbalanced datasets               | -1 to 1     | High                | $ \\frac{(TP \\cdot TN - FP \\cdot FN)}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}} $            | Handles all confusion matrix terms, good for imbalance      | Slightly complex formula                                    | Highly imbalanced binary classification                     |\n",
    "| **ROC-AUC**          | Ranking ability across thresholds        | 0 to 1      | High                | AUC of ROC curve (TPR vs FPR)                                                           | Threshold-independent, visual tool                          | Misleading in extreme imbalance                             | Classifier ranking and threshold selection                  |\n",
    "| **PR-AUC**           | Imbalanced classification                | 0 to 1      | High                | AUC of Precision vs Recall curve                                                        | Better than ROC-AUC for imbalance                           | Hard to interpret without plot                             | Imbalanced datasets focusing on positive class              |\n",
    "| **Log Loss**         | Probabilistic binary classification       | 0 to âˆž      | High                | $ -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)] $              | Penalises wrong confident predictions                       | Sensitive to outliers, only for probabilistic models        | Probabilistic binary classification                         |\n",
    "\n",
    "---\n",
    "\n",
    "### a. **Accuracy**\n",
    "\n",
    "- **Definition**:  \n",
    "  Proportion of correct predictions. Measures how many predictions the model got **correct** out of all predictions made.\n",
    "  \n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "  $$\n",
    "\n",
    "- **When to Use**:  \n",
    "  - When the dataset has **balanced class distribution**.\n",
    "  - Simple binary classification problems.\n",
    "\n",
    "- **Real-life Example**:  \n",
    "  Predicting whether a transaction is fraudulent when fraud and non-fraud cases are nearly equal.\n",
    "\n",
    "- **Pros**:\n",
    "  - Easy to understand and quick to compute.\n",
    "\n",
    "- **Cons**:\n",
    "  - **Misleading in imbalanced datasets**. For example, if 95 out of 100 samples are \"Not Fraud\", predicting all as \"Not Fraud\" gives 95% accuracy but **0% usefulness**.\n",
    "\n",
    "---\n",
    "\n",
    "### b. **Precision (Positive Predictive Value)**\n",
    "\n",
    "- **Definition**:  \n",
    "  Out of all predicted **positive cases**, how many were **actually positive**.\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "  $$\n",
    "\n",
    "- **When to Use**:  \n",
    "  - When **false positives** are **costly or risky**.\n",
    "\n",
    "- **Real-life Example**:  \n",
    "  Email spam detection. Marking an important email as spam (FP) is worse than missing one spam email.\n",
    "\n",
    "- **Pros**:\n",
    "  - Tells us how **reliable** our positive predictions are.\n",
    "\n",
    "- **Cons**:\n",
    "  - Doesnâ€™t consider false negatives (what we missed).\n",
    "\n",
    "---\n",
    "\n",
    "### c. **Recall (Sensitivity or True Positive Rate)**\n",
    "\n",
    "- **Definition**:  \n",
    "  Out of all **actual positive cases**, how many were **correctly identified** by the model.\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "  $$\n",
    "\n",
    "- **When to Use**:  \n",
    "  - When **missing positive cases** is **dangerous or expensive**.\n",
    "\n",
    "- **Real-life Example**:  \n",
    "  Cancer detection. Missing a cancer case (FN) could be life-threatening.\n",
    "\n",
    "- **Pros**:\n",
    "  - Highlights the modelâ€™s **completeness** in detecting positives.\n",
    "\n",
    "- **Cons**:\n",
    "  - Doesnâ€™t care about false positives.\n",
    "\n",
    "---\n",
    "\n",
    "### d. **Specificity (True Negative Rate)**\n",
    "\n",
    "- **Definition**:  \n",
    "  Out of all **actual negative cases**, how many were **correctly identified** as negative.\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "  $$\n",
    "\n",
    "- **When to Use**:  \n",
    "  - When itâ€™s critical to **avoid false positives**.\n",
    "\n",
    "- **Real-life Example**:  \n",
    "  Medical tests: You donâ€™t want to falsely diagnose healthy people as sick.\n",
    "\n",
    "- **Pros**:\n",
    "  - Complements Recall (which focuses on positives).\n",
    "\n",
    "- **Cons**:\n",
    "  - Often ignored in standard metrics like Accuracy and F1.\n",
    "\n",
    "---\n",
    "\n",
    "### e. **F1-Score**\n",
    "\n",
    "- **Definition**:  \n",
    "  **Harmonic mean** of Precision and Recall. It balances both, giving a **single metric**.\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "  $$\n",
    "\n",
    "- **When to Use**:  \n",
    "  - When you need a balance between Precision and Recall.\n",
    "  - Especially useful in **imbalanced classification problems**.\n",
    "\n",
    "- **Real-life Example**:  \n",
    "  In fraud detection, you want to **detect frauds (recall)** and also make sure the detected frauds are **truly fraud (precision)**.\n",
    "\n",
    "- **Pros**:\n",
    "  - Balances two important metrics into one score.\n",
    "\n",
    "- **Cons**:\n",
    "  - Not intuitive like Accuracy, and harder to explain.\n",
    "\n",
    "---\n",
    "\n",
    "### f. **F<sub>Î²</sub>-Score**\n",
    "\n",
    "- **Definition**:  \n",
    "  Generalized form of F1-score that allows **weighting Recall more than Precision**, or vice versa.\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{Precision \\cdot Recall}{\\beta^2 \\cdot Precision + Recall}\n",
    "  $$\n",
    "\n",
    "- **Interpretation**:\n",
    "  - **Î² = 1** â†’ Equal weight to Precision and Recall (F1).\n",
    "  - **Î² > 1** â†’ More weight to **Recall**.\n",
    "  - **Î² < 1** â†’ More weight to **Precision**.\n",
    "\n",
    "- **Real-life Examples**:\n",
    "  - **Î² = 2** (Recall Focus): Disease detection, where **missing** cases is worse.\n",
    "  - **Î² = 0.5** (Precision Focus): Spam filters, where **wrongly tagging** a mail as spam is worse.\n",
    "\n",
    "- **Pros**:\n",
    "  - Flexible metric depending on problem sensitivity.\n",
    "\n",
    "- **Cons**:\n",
    "  - Needs domain knowledge to choose the right Î².\n",
    "\n",
    "---\n",
    "\n",
    "### g. **Confusion Matrix**\n",
    "\n",
    "|                             | Predicted Positive | Predicted Negative |\n",
    "|-----------------------------|--------------------|--------------------|\n",
    "| **Actual Positive**         | TP (True Positive) | FN (False Negative) |\n",
    "| **Actual Negative**         | FP (False Positive) | TN (True Negative)  |\n",
    "\n",
    "- **Definition**:  \n",
    "  A **2Ã—2 matrix** that breaks down predictions into **correct and incorrect** classes.\n",
    "\n",
    "- **When to Use**:  \n",
    "  - Always useful for **diagnosing model errors**.\n",
    "\n",
    "- **Real-life Example**:  \n",
    "  Helps understand whether a model **missed actual cases** or **flagged incorrect ones**.\n",
    "\n",
    "- **Pros**:\n",
    "  - Provides a **complete breakdown** of model performance.\n",
    "\n",
    "- **Cons**:\n",
    "  - Not a single value; needs further analysis for summary.\n",
    "\n",
    "---\n",
    "\n",
    "### h. **Balanced Accuracy**\n",
    "\n",
    "- **Definition**:  \n",
    "  Average of **Recall** (True Positive Rate) and **Specificity** (True Negative Rate).\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{Balanced Accuracy} = \\frac{1}{2} \\left( \\frac{TP}{TP + FN} + \\frac{TN}{TN + FP} \\right)\n",
    "  $$\n",
    "\n",
    "- **When to Use**:  \n",
    "  - For **imbalanced datasets** where one class dominates.\n",
    "\n",
    "- **Pros**:\n",
    "  - Fairly evaluates both classes.\n",
    "  \n",
    "- **Cons**:\n",
    "  - Less popular than F1, Accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### i. **Matthews Correlation Coefficient (MCC)**\n",
    "\n",
    "- **Definition**:  \n",
    "  Measures the **correlation** between actual and predicted classifications.\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  MCC = \\frac{(TP \\cdot TN) - (FP \\cdot FN)}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
    "  $$\n",
    "\n",
    "- **Score Range**:\n",
    "  - +1: Perfect prediction\n",
    "  - 0: Random prediction\n",
    "  - â€“1: Complete disagreement\n",
    "\n",
    "- **When to Use**:  \n",
    "  - Especially useful for **imbalanced datasets**.\n",
    "\n",
    "- **Pros**:\n",
    "  - Takes all confusion matrix values into account.\n",
    "  - Considered **one of the most reliable binary metrics**.\n",
    "\n",
    "- **Cons**:\n",
    "  - Slightly complex and not very intuitive.\n",
    "\n",
    "---\n",
    "\n",
    "### j. **ROC Curve & AUC (Receiver Operating Characteristic â€“ Area Under Curve)**\n",
    "\n",
    "- **Definition**:  \n",
    "  ROC is a curve of **True Positive Rate (Recall)** vs. **False Positive Rate** at various thresholds.  \n",
    "  **AUC** is the **area under that curve**.\n",
    "\n",
    "- **Formula**:  \n",
    "  No single formula, but AUC is computed via integration.\n",
    "\n",
    "- **Interpretation**:\n",
    "  - **AUC = 1**: Perfect model\n",
    "  - **AUC = 0.5**: No better than random guessing\n",
    "\n",
    "- **When to Use**:  \n",
    "  - When you want to assess modelâ€™s **ranking ability**, especially with **probabilistic outputs**.\n",
    "\n",
    "- **Real-life Example**:  \n",
    "  Credit scoring, disease risk scores.\n",
    "\n",
    "- **Pros**:\n",
    "  - Threshold-independent.\n",
    "  \n",
    "- **Cons**:\n",
    "  - Can be **misleading with highly imbalanced data**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### k. **PR Curve & PR AUC (Precision-Recall Curve â€“ Area Under Curve)**\n",
    "\n",
    "- **Definition**:  \n",
    "  The PR curve is a plot of **Precision vs Recall** for various thresholds, and **PR AUC** is the area under this curve.\n",
    "\n",
    "- **Formula**:  \n",
    "  No single formula, but AUC is computed via integration.\n",
    "\n",
    "- **Interpretation**:\n",
    "  - **PR AUC = 1**: Perfect model\n",
    "  - **PR AUC = 0**: No better than random guessing\n",
    "\n",
    "- **When to Use**:  \n",
    "  - Especially useful in **imbalanced datasets** where the positive class is of more interest.\n",
    "\n",
    "- **Real-life Example**:  \n",
    "  Medical testing (e.g., detecting a rare disease), fraud detection.\n",
    "\n",
    "- **Pros**:\n",
    "  - Focuses on the performance of the model for the positive class.\n",
    "  \n",
    "- **Cons**:\n",
    "  - Can be difficult to interpret without visualizing the full curve.\n",
    "\n",
    "---\n",
    "\n",
    "### l. **Log Loss (Logarithmic Loss or Cross-Entropy Loss)**\n",
    "\n",
    "- **Definition**:  \n",
    "  A loss function that measures the uncertainty of the modelâ€™s predictions. It penalizes wrong predictions based on how confident the model was in its incorrect predictions.\n",
    "\n",
    "- **Formula**:  \n",
    "  $$ \n",
    "  \\text{Log Loss} = - \\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]\n",
    "  $$  \n",
    "  where:\n",
    "  - $ y_i $ = Actual class (0 or 1)\n",
    "  - $ p_i $ = Predicted probability for the positive class\n",
    "\n",
    "- **When to Use**:  \n",
    "  - For **probabilistic classifiers** (e.g., logistic regression, neural networks).\n",
    "\n",
    "- **Real-life Example**:  \n",
    "  Binary classification problems with probabilistic outputs, such as predicting whether an email is spam or not, where predictions are given as probabilities.\n",
    "\n",
    "- **Pros**:\n",
    "  - Provides a **continuous** measure of performance, not just binary outputs.\n",
    "  - Penalizes models that are confident but wrong, helping improve the quality of predictions.\n",
    "\n",
    "- **Cons**:\n",
    "  - Sensitive to outliers and can give high penalties for very confident incorrect predictions.\n",
    "  - Not as interpretable as other metrics like Accuracy or F1-Score.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
