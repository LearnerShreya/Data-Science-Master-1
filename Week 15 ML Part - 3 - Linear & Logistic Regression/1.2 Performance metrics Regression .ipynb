{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c45cd12",
   "metadata": {},
   "source": [
    "# üìä Performance Metrics in Regression Model\n",
    "\n",
    "Performance metrics help assess how well a machine learning model performs. The choice of metric depends on the **problem type**‚Äîwhether it's classification, regression, or clustering. Below is a breakdown of key metrics used in different types of problems, with **formulas**, **definitions**, and **advantages/disadvantages**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **2. Regression Metrics**\n",
    "Used when the output is **continuous** (e.g., predicting temperature, sales, house prices).\n",
    "\n",
    "### a. **Mean Absolute Error (MAE)**\n",
    "- **Definition**: Average of absolute errors  \n",
    "  $$\n",
    "  MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "  $$\n",
    "  - $n$ = number of observations  \n",
    "  - $y_i$ = actual value  \n",
    "  - $\\hat{y}_i$ = predicted value\n",
    "- **Advantages**:\n",
    "  - Easy to interpret.\n",
    "  - **Same unit as target**.\n",
    "- **Disadvantages**:\n",
    "  - Doesn't penalize large errors.\n",
    "- **Best Use Case**: When you want a **simple error metric**.\n",
    "\n",
    "---\n",
    "\n",
    "### b. **Mean Squared Error (MSE)**\n",
    "- **Definition**: Average of squared errors  \n",
    "  $$\n",
    "  MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "  - Same as above\n",
    "- **Advantages**:\n",
    "  - Penalizes **large errors** more.\n",
    "- **Disadvantages**:\n",
    "  - Less interpretable due to squaring.\n",
    "- **Best Use Case**: When you want to **penalize large deviations**.\n",
    "\n",
    "---\n",
    "\n",
    "### c. **Root Mean Squared Error (RMSE)**\n",
    "- **Definition**: Square root of MSE  \n",
    "  $$\n",
    "  RMSE = \\sqrt{MSE}\n",
    "  $$\n",
    "- **Advantages**:\n",
    "  - **Same unit as target**.\n",
    "  - Balances error size.\n",
    "- **Disadvantages**:\n",
    "  - Sensitive to **outliers**.\n",
    "- **Best Use Case**: For general-purpose **regression evaluation**.\n",
    "\n",
    "---\n",
    "\n",
    "### d. **R¬≤ Score (Coefficient of Determination)**\n",
    "- **Definition**: Proportion of variance explained by the model  \n",
    "  $$\n",
    "  R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n",
    "  $$\n",
    "  - $SS_{res} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$ ‚Üí residual sum of squares  \n",
    "  - $SS_{tot} = \\sum_{i=1}^n (y_i - \\bar{y})^2$ ‚Üí total sum of squares  \n",
    "  - $\\bar{y}$ = mean of actual values\n",
    "- **Advantages**:\n",
    "  - Measures how well the model explains the data.\n",
    "- **Disadvantages**:\n",
    "  - Can be misleading for **nonlinear** models or **overfitted** models.\n",
    "- **Best Use Case**: When evaluating **goodness of fit**.\n",
    "\n",
    "---\n",
    "\n",
    "### e. **Adjusted R¬≤ Score**\n",
    "- **Definition**: R¬≤ adjusted for the number of predictors  \n",
    "  $$\n",
    "  \\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right)\n",
    "  $$\n",
    "  - $n$ = number of observations  \n",
    "  - $k$ = number of independent predictors\n",
    "- **Advantages**:\n",
    "  - Penalizes irrelevant features.\n",
    "  - Adjusts for **model complexity**.\n",
    "- **Disadvantages**:\n",
    "  - Slightly more complex to compute.\n",
    "- **Best Use Case**: Comparing models with **multiple features**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **3. Clustering Metrics**\n",
    "Used in **unsupervised learning** (e.g., grouping customers or documents).\n",
    "\n",
    "### a. **Silhouette Score**\n",
    "- **Definition**: Measures how similar a point is to its own cluster compared to other clusters  \n",
    "  $$\n",
    "  \\text{Silhouette} \\in [-1, 1]\n",
    "  $$\n",
    "- **Advantages**:\n",
    "  - Intuitive: closer to 1 is better.\n",
    "- **Disadvantages**:\n",
    "  - Computationally expensive for large datasets.\n",
    "- **Best Use Case**: Evaluating **cohesion** and **separation** in clustering.\n",
    "\n",
    "---\n",
    "\n",
    "### b. **Davies-Bouldin Index**\n",
    "- **Definition**: Measures average similarity between each cluster and the most similar one.\n",
    "- **Advantages**:\n",
    "  - **Lower scores** indicate better clustering.\n",
    "- **Disadvantages**:\n",
    "  - Doesn't handle **irregular shapes** of clusters well.\n",
    "- **Best Use Case**: For comparing clustering **tightness and separation**.\n",
    "\n",
    "---\n",
    "\n",
    "### c. **Adjusted Rand Index (ARI)**\n",
    "- **Definition**: Compares predicted clusters against ground truth.\n",
    "- **Advantages**:\n",
    "  - **Accounts for chance** in clustering.\n",
    "  - Corrects for **random clustering**.\n",
    "- **Disadvantages**:\n",
    "  - Requires **true labels**.\n",
    "- **Best Use Case**: When you have **ground truth labels** for clustering.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Summary Table\n",
    "\n",
    "| Problem Type     | Metric               | Formula                                          | Advantages                                    | Disadvantages                              | Best Use Case                             |\n",
    "|------------------|----------------------|--------------------------------------------------|-----------------------------------------------|--------------------------------------------|--------------------------------------------|\n",
    "| Classification   | Accuracy             | $ \\frac{TP + TN}{TP + TN + FP + FN} $            | Easy to interpret                             | Misleading on imbalanced data              | Balanced datasets                         |\n",
    "|                  | Precision            | $ \\frac{TP}{TP + FP} $                           | Focus on correct positives                    | Ignores false negatives                    | Minimize false positives (e.g., spam)     |\n",
    "|                  | Recall               | $ \\frac{TP}{TP + FN} $                           | Captures all actual positives                 | Ignores false positives                    | Minimize false negatives (e.g., medical)  |\n",
    "|                  | F1-Score             | $ 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $ | Balances precision & recall | Hard to interpret intuitively              | Imbalanced datasets                       |\n",
    "|                  | ROC-AUC              | -                                                | Evaluates without threshold                  | Misleading on imbalanced data              | Probabilistic classifiers                 |\n",
    "| Regression       | MAE                  | $ \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i| $ | Simple and interpretable                      | Doesn‚Äôt penalize large errors              | Simple error analysis                     |\n",
    "|                  | MSE                  | $ \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $ | Penalizes large errors                        | Less interpretable                         | Model optimization                        |\n",
    "|                  | RMSE                 | $ \\sqrt{MSE} $                                   | Same unit as target, penalizes large errors   | Sensitive to outliers                      | General-purpose regression metric         |\n",
    "|                  | R¬≤ Score             | $ 1 - \\frac{SS_{res}}{SS_{tot}} $                | Measures explained variance                   | Overestimates performance                  | Fit evaluation                            |\n",
    "|                  | Adjusted R¬≤ Score    | $ 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right) $ | Penalizes irrelevant features                 | Slightly complex                           | Feature selection and model comparison    |\n",
    "| Clustering       | Silhouette Score     | $ \\in [-1, 1] $                                  | No labels required, easy to interpret         | Slow for large datasets                    | Evaluating cohesion/separation            |\n",
    "|                  | Davies-Bouldin Index | -                                                | Simple, efficient                             | May fail on irregular clusters             | Model selection in clustering             |\n",
    "|                  | Adjusted Rand Index  | -                                                | Accounts for random labeling                  | Needs true labels                          | Comparing to true labels in clustering    |\n",
    "\n",
    "--- \n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787080ee",
   "metadata": {},
   "source": [
    "Absolutely, Shreya! Here's a detailed and well-structured üìò **Regression Metrics** note in the **same style** as your classification one, with ‚úÖ formulas, üîç use-cases, üìà advantages/disadvantages, and üß† best-fit scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "# üìâ Performance Metrics in Regression Models\n",
    "\n",
    "Regression metrics are used when the model‚Äôs **output is continuous** (e.g., house price, temperature, stock value). These metrics help measure **how far the model's predictions are from actual values**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Common Use-Cases of Regression:\n",
    "- Predicting house prices\n",
    "- Forecasting sales or demand\n",
    "- Estimating crop yields\n",
    "- Predicting blood sugar levels\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Metric Comparison Table\n",
    "\n",
    "| Metric               | Best Use Case                              | Range            | Formula Complexity |\n",
    "|----------------------|--------------------------------------------|------------------|---------------------|\n",
    "| MAE                 | Average error size                          | 0 to ‚àû           | Low                 |\n",
    "| MSE                 | Penalises large errors                      | 0 to ‚àû           | Low                 |\n",
    "| RMSE                | Same unit as target, penalises large errors | 0 to ‚àû           | Medium              |\n",
    "| R¬≤ (R-squared)      | Proportion of variance explained            | -‚àû to 1          | Medium              |\n",
    "| Adjusted R¬≤         | Corrects R¬≤ for multiple features           | -‚àû to 1          | Medium              |\n",
    "| MAPE                | Error relative to actual value (%)          | 0 to ‚àû           | Medium              |\n",
    "| MSLE                | Penalises under-prediction less             | 0 to ‚àû           | Medium              |\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Regression Metrics Summary Table\n",
    "\n",
    "| Metric               | Formula                                                                                   | Advantages                                          | Disadvantages                                         | Best Use Case                                               |\n",
    "|----------------------|-------------------------------------------------------------------------------------------|-----------------------------------------------------|-------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **MAE**              | $ \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $                                          | Easy to interpret, equal weight to all errors        | Doesn‚Äôt penalise large errors                          | General regression tasks                                     |\n",
    "| **MSE**              | $ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $                                         | Emphasises large errors                             | Sensitive to outliers                                 | When large errors are more problematic                      |\n",
    "| **RMSE**             | $ \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $                                 | Same units as actual values                         | Same sensitivity to outliers as MSE                   | House price, medical dosage prediction                      |\n",
    "| **R¬≤ Score**         | $ 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} $                           | Explains variance, intuitive                        | Can be misleading with few data points                | Model explanation and feature evaluation                    |\n",
    "| **Adjusted R¬≤**      | $ 1 - (1 - R^2)\\frac{n - 1}{n - p - 1} $                                                  | Adjusts R¬≤ for number of predictors                 | Complex, can be < R¬≤                                  | Multiple linear regression                                  |\n",
    "| **MAPE**             | $ \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| $             | Easy to interpret as percentage                     | Undefined when $y_i = 0$                              | Business forecasts, demand prediction                       |\n",
    "| **MSLE**             | $ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\log(1 + y_i) - \\log(1 + \\hat{y}_i) \\right)^2 $       | Better for lower values                             | Can't handle negative values                          | Growth rate prediction, exponential trends                  |\n",
    "\n",
    "---\n",
    "\n",
    "### a. **MAE (Mean Absolute Error)**\n",
    "\n",
    "- **Definition**:  \n",
    "  Measures the average magnitude of errors in a set of predictions, without considering direction (positive/negative).\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "  $$\n",
    "\n",
    "- **Pros**:\n",
    "  - Intuitive and simple.\n",
    "  - Equal weight to all errors.\n",
    "\n",
    "- **Cons**:\n",
    "  - Doesn‚Äôt highlight large errors.\n",
    "\n",
    "- **Use Case**:  \n",
    "  Basic regression tasks like estimating sales where equal weight to all mistakes is acceptable.\n",
    "\n",
    "---\n",
    "\n",
    "### b. **MSE (Mean Squared Error)**\n",
    "\n",
    "- **Definition**:  \n",
    "  Squared average of the prediction errors. Emphasizes larger errors more due to squaring.\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "\n",
    "- **Pros**:\n",
    "  - Penalizes large errors ‚Üí useful when big mistakes are costly.\n",
    "\n",
    "- **Cons**:\n",
    "  - Not in the same unit as the target.\n",
    "  - Sensitive to outliers.\n",
    "\n",
    "- **Use Case**:  \n",
    "  Model tuning, where penalising large errors (like in energy consumption) is key.\n",
    "\n",
    "---\n",
    "\n",
    "### c. **RMSE (Root Mean Squared Error)**\n",
    "\n",
    "- **Definition**:  \n",
    "  Square root of MSE ‚Äî brings the error to the same unit as the original output.\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "  $$\n",
    "\n",
    "- **Pros**:\n",
    "  - Interpretable in same units as the target.\n",
    "  - Highlights large errors.\n",
    "\n",
    "- **Cons**:\n",
    "  - Still sensitive to outliers.\n",
    "\n",
    "- **Use Case**:  \n",
    "  Projects involving physical quantities (e.g., predicting house area in square feet).\n",
    "\n",
    "---\n",
    "\n",
    "### d. **R¬≤ (Coefficient of Determination)**\n",
    "\n",
    "- **Definition**:  \n",
    "  Proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
    "  $$\n",
    "\n",
    "- **Pros**:\n",
    "  - Indicates how well the model fits.\n",
    "  - Common and interpretable.\n",
    "\n",
    "- **Cons**:\n",
    "  - Can be misleading with few observations.\n",
    "  - Doesn‚Äôt indicate error size.\n",
    "\n",
    "- **Use Case**:  \n",
    "  When explaining variance is crucial (e.g., effect of education on income).\n",
    "\n",
    "---\n",
    "\n",
    "### e. **Adjusted R¬≤**\n",
    "\n",
    "- **Definition**:  \n",
    "  Adjusts R¬≤ to account for number of predictors. Prevents overfitting by penalising unnecessary features.\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{Adjusted } R^2 = 1 - (1 - R^2) \\cdot \\frac{n - 1}{n - p - 1}\n",
    "  $$\n",
    "\n",
    "- **Pros**:\n",
    "  - Reliable for models with many features.\n",
    "\n",
    "- **Cons**:\n",
    "  - Complex to compute and interpret.\n",
    "\n",
    "- **Use Case**:  \n",
    "  Multivariate regression models (e.g., predicting house price from multiple features).\n",
    "\n",
    "---\n",
    "\n",
    "### f. **MAPE (Mean Absolute Percentage Error)**\n",
    "\n",
    "- **Definition**:  \n",
    "  Measures average absolute percent error between actual and predicted values.\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{MAPE} = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|\n",
    "  $$\n",
    "\n",
    "- **Pros**:\n",
    "  - Expresses error as a percentage ‚Üí easy to interpret.\n",
    "\n",
    "- **Cons**:\n",
    "  - Undefined when actual value is 0.\n",
    "  - Biased toward low values.\n",
    "\n",
    "- **Use Case**:  \n",
    "  Forecasting revenue or sales in business analytics.\n",
    "\n",
    "---\n",
    "\n",
    "### g. **MSLE (Mean Squared Logarithmic Error)**\n",
    "\n",
    "- **Definition**:  \n",
    "  Penalizes under-predictions less than over-predictions. Suitable when the target values vary over several orders of magnitude.\n",
    "\n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\text{MSLE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\log(1 + y_i) - \\log(1 + \\hat{y}_i) \\right)^2\n",
    "  $$\n",
    "\n",
    "- **Pros**:\n",
    "  - Better for comparing relative error.\n",
    "  - Less sensitive to large differences in higher ranges.\n",
    "\n",
    "- **Cons**:\n",
    "  - Cannot be used with negative values.\n",
    "\n",
    "- **Use Case**:  \n",
    "  Growth modeling, e.g., predicting user adoption over time.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to create **visuals** like the confusion matrix or any **summary flashcards** for revision?\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37323861",
   "metadata": {},
   "source": [
    "Certainly! Here's a detailed overview of **Regression** types, including the **summary tables** and key metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## **üî∏ Types of Regression**\n",
    "\n",
    "### 1. **Linear Regression**\n",
    "\n",
    "Linear regression is used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n",
    "\n",
    "- **Formula**: $ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n $\n",
    "  \n",
    "- **Best Use Case**: When the dependent variable has a linear relationship with the independent variables.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Simple to implement.\n",
    "  - Easy to interpret coefficients.\n",
    "  - Works well for small to medium datasets.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Assumes a linear relationship.\n",
    "  - Sensitive to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Multiple Linear Regression**\n",
    "\n",
    "Multiple linear regression extends linear regression to predict a dependent variable based on two or more independent variables.\n",
    "\n",
    "- **Formula**: $ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n $\n",
    "\n",
    "- **Best Use Case**: Predicting a single dependent variable when there are multiple predictors.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Can handle multiple predictors.\n",
    "  - More accurate than simple linear regression with multiple influencing factors.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Prone to overfitting.\n",
    "  - Needs large datasets to work well.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Polynomial Regression**\n",
    "\n",
    "Polynomial regression is a form of regression analysis where the relationship between the independent and dependent variables is modeled as an nth degree polynomial.\n",
    "\n",
    "- **Formula**: $ Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + ... + \\beta_n X^n $\n",
    "\n",
    "- **Best Use Case**: When data shows a non-linear relationship that can be approximated by a polynomial.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Can model more complex relationships than linear regression.\n",
    "  - Flexible for curved data patterns.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - High degree polynomials can lead to overfitting.\n",
    "  - Complex and difficult to interpret.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Ridge Regression (L2 Regularization)**\n",
    "\n",
    "Ridge regression adds a penalty term (L2 regularization) to the least squares objective function to prevent overfitting by shrinking the coefficients.\n",
    "\n",
    "- **Formula**: $ \\hat{\\beta} = \\text{argmin} \\left( \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{m} \\beta_j^2 \\right) $\n",
    "\n",
    "- **Best Use Case**: When dealing with multicollinearity or high-dimensional data.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Helps reduce model complexity.\n",
    "  - Works well when predictors are highly correlated.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - May underfit if lambda (penalty term) is too large.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Lasso Regression (L1 Regularization)**\n",
    "\n",
    "Lasso regression is similar to ridge regression but uses L1 regularization, which can force some coefficient estimates to zero, effectively performing feature selection.\n",
    "\n",
    "- **Formula**: $ \\hat{\\beta} = \\text{argmin} \\left( \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{m} |\\beta_j| \\right) $\n",
    "\n",
    "- **Best Use Case**: When there is a need for feature selection or dealing with sparse data.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Helps with feature selection.\n",
    "  - Can handle high-dimensional datasets.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Prone to underfitting.\n",
    "  - Difficult to choose the right value for lambda.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Elastic Net Regression**\n",
    "\n",
    "Elastic Net is a combination of Lasso and Ridge regression, which balances L1 and L2 regularization. It is particularly useful when there are correlations between features.\n",
    "\n",
    "- **Formula**: $ \\hat{\\beta} = \\text{argmin} \\left( \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\left( \\alpha \\sum_{j=1}^{m} |\\beta_j| + \\frac{1 - \\alpha}{2} \\sum_{j=1}^{m} \\beta_j^2 \\right) \\right) $\n",
    "\n",
    "- **Best Use Case**: When dealing with highly correlated features or a mix of regularization needs.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Combines the benefits of both Ridge and Lasso.\n",
    "  - Flexible and robust for many types of regression problems.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Needs careful tuning of both regularization parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## **üìä Regression Metrics Summary Table**\n",
    "\n",
    "| Metric                | Formula                                                                                             | Advantages                                                | Disadvantages                                               | Best Use Case                                               |\n",
    "|-----------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| **Mean Absolute Error (MAE)** | $ \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}| $                                           | Simple, easy to interpret                                  | Sensitive to outliers                                        | General regression tasks                                    |\n",
    "| **Mean Squared Error (MSE)** | $ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 $                                                | Penalizes larger errors more than MAE                     | Sensitive to outliers                                        | Tasks with a focus on minimizing large errors                |\n",
    "| **Root Mean Squared Error (RMSE)** | $ \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 } $                                   | Easy to interpret (in same units as target)               | Sensitive to outliers                                        | Tasks where large errors should be penalized more            |\n",
    "| **R-Squared (R¬≤)**    | $ 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $                | Measures goodness of fit, easy to interpret               | Doesn‚Äôt work well with non-linear models                    | Evaluating how well a model fits the data                    |\n",
    "| **Adjusted R-Squared**| $ 1 - (1 - R^2) \\cdot \\frac{n-1}{n-p-1} $                                                           | Adjusts for the number of predictors, handles overfitting | Still sensitive to outliers                                  | Linear regression with multiple predictors                   |\n",
    "| **Mean Absolute Percentage Error (MAPE)** | $ \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y_i}}{y_i} \\right| $                 | Interpretable percentage errors                           | Can be misleading if actual values are near zero              | Forecasting, time series                                   |\n",
    "| **Huber Loss**        | $ \\frac{1}{n} \\sum_{i=1}^{n} \\delta \\cdot \\left| y_i - \\hat{y_i} \\right| \\text{ for small errors} $ | Robust to outliers, combines MAE and MSE                   | Harder to interpret                                          | Regression tasks with outliers                               |\n",
    "\n",
    "---\n",
    "\n",
    "## **üìä Regression Metrics Comparison Table**\n",
    "\n",
    "| Metric                | Best Use Case                             | Range       | Formula Complexity |\n",
    "|-----------------------|-------------------------------------------|-------------|---------------------|\n",
    "| **Mean Absolute Error (MAE)** | Tasks with smaller errors important    | 0 to ‚àû      | Low                 |\n",
    "| **Mean Squared Error (MSE)** | Minimizing large errors                 | 0 to ‚àû      | Low                 |\n",
    "| **Root Mean Squared Error (RMSE)** | Tasks where larger errors should be penalized | 0 to ‚àû      | Low                 |\n",
    "| **R-Squared (R¬≤)**    | Evaluating goodness of fit                | 0 to 1      | Low                 |\n",
    "| **Adjusted R-Squared**| Model evaluation with multiple predictors | 0 to 1      | Medium              |\n",
    "| **Mean Absolute Percentage Error (MAPE)** | Time series, forecasting | 0 to ‚àû      | Medium              |\n",
    "| **Huber Loss**        | Regression with outliers                  | 0 to ‚àû      | Medium              |\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Clustering Metrics Summary Table**\n",
    "\n",
    "The following table summarizes key clustering evaluation metrics (for comparison between clusters) which you might find useful when selecting the best regression model based on the task.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like further clarification on any of the metrics or regression types!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
