{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf793ea",
   "metadata": {},
   "source": [
    "# 🌟 **Regression in Machine Learning – Detailed Notes**\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 **What is Regression?**\n",
    "\n",
    "Regression is a **supervised learning technique** used in machine learning to **predict continuous (real-valued) outcomes** based on input data. It involves estimating the relationships among variables. In regression problems, the output variable is numerical (e.g., price, age, temperature), unlike classification where outputs are categorical (e.g., spam or not spam).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 **Why Use Regression?**\n",
    "\n",
    "- To understand relationships between variables.\n",
    "- To forecast or predict future outcomes.\n",
    "- To identify trends and patterns.\n",
    "- To perform risk analysis or make informed decisions based on data.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 **Types of Regression in Machine Learning – In Detail**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Simple Linear Regression**\n",
    "\n",
    "#### ✅ Definition:\n",
    "Simple Linear Regression models the relationship between a **single independent variable** and a **dependent variable** by fitting a straight line (linear) to the data.\n",
    "\n",
    "#### 🧮 Mathematical Model:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "$$\n",
    "\n",
    "- $y$ = Target (dependent) variable  \n",
    "- $x$ = Predictor (independent) variable  \n",
    "- $\\beta_0$ = Intercept (value of $y$ when $x = 0$)  \n",
    "- $\\beta_1$ = Slope (rate of change of $y$ with respect to $x$)  \n",
    "- $\\epsilon$ = Error/residual term (difference between actual and predicted value)\n",
    "\n",
    "#### 🔍 Use Case:\n",
    "- Predicting salary based on years of experience  \n",
    "- Estimating house price based on square footage\n",
    "\n",
    "#### 🧱 Assumptions:\n",
    "- Linear relationship between $x$ and $y$\n",
    "- Independence of residuals\n",
    "- Constant variance of residuals (homoscedasticity)\n",
    "- Normally distributed residuals\n",
    "\n",
    "#### ✅ Advantages:\n",
    "- Easy to implement and interpret\n",
    "- Works well when the relationship is truly linear\n",
    "\n",
    "#### ❌ Limitations:\n",
    "- Only works with one feature\n",
    "- Not suitable for non-linear relationships\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Multiple Linear Regression**\n",
    "\n",
    "#### ✅ Definition:\n",
    "Extends simple linear regression to **multiple independent variables**.\n",
    "\n",
    "#### 🧮 Mathematical Model:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n",
    "$$\n",
    "\n",
    "- $x_1, x_2, \\dots, x_n$ = Independent variables (features)\n",
    "\n",
    "#### 🔍 Use Case:\n",
    "- Predicting house price based on area, number of rooms, location, etc.\n",
    "- Predicting student marks based on study time, sleep hours, etc.\n",
    "\n",
    "#### 🧱 Assumptions:\n",
    "Same as simple linear regression, with the additional assumption:\n",
    "- No multicollinearity (independent variables shouldn't be highly correlated)\n",
    "\n",
    "#### ✅ Advantages:\n",
    "- Can handle complex relationships with many features\n",
    "\n",
    "#### ❌ Limitations:\n",
    "- Prone to overfitting with irrelevant features\n",
    "- Sensitive to multicollinearity\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Polynomial Regression**\n",
    "\n",
    "#### ✅ Definition:\n",
    "A type of regression where the relationship between the independent variable and the dependent variable is modeled as an **nth-degree polynomial**.\n",
    "\n",
    "#### 🧮 Mathematical Model:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\dots + \\beta_n x^n + \\epsilon\n",
    "$$\n",
    "\n",
    "#### 🔍 Use Case:\n",
    "- Modeling population growth\n",
    "- Predicting trajectories (e.g., ball thrown in the air)\n",
    "\n",
    "#### 🧱 Assumptions:\n",
    "- Relationship between variables is polynomial\n",
    "- Same as linear regression assumptions (except linearity in features)\n",
    "\n",
    "#### ✅ Advantages:\n",
    "- Can model non-linear relationships\n",
    "\n",
    "#### ❌ Limitations:\n",
    "- High-degree polynomials can overfit\n",
    "- Interpretability decreases with increasing degree\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Ridge Regression (L2 Regularization)**\n",
    "\n",
    "#### ✅ Definition:\n",
    "Linear regression with **L2 regularization**. Adds a penalty for large coefficients to reduce model complexity and overfitting.\n",
    "\n",
    "#### 🧮 Loss Function:\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "$$\n",
    "\n",
    "- $\\lambda$ = Regularization strength  \n",
    "- Larger $\\lambda$ leads to more shrinkage\n",
    "\n",
    "#### 🔍 Use Case:\n",
    "- Situations with multicollinearity or many features\n",
    "\n",
    "#### ✅ Advantages:\n",
    "- Reduces overfitting\n",
    "- Improves model generalization\n",
    "\n",
    "#### ❌ Limitations:\n",
    "- All coefficients are reduced, but **none are eliminated**\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Lasso Regression (L1 Regularization)**\n",
    "\n",
    "#### ✅ Definition:\n",
    "Linear regression with **L1 regularization**, which can reduce some coefficients to exactly zero, performing **feature selection**.\n",
    "\n",
    "#### 🧮 Loss Function:\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "$$\n",
    "\n",
    "#### 🔍 Use Case:\n",
    "- Feature selection when you suspect only a few features are important\n",
    "\n",
    "#### ✅ Advantages:\n",
    "- Automatic feature selection\n",
    "- Sparse solutions (some coefficients are zero)\n",
    "\n",
    "#### ❌ Limitations:\n",
    "- Can behave erratically when variables are highly correlated\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Elastic Net Regression**\n",
    "\n",
    "#### ✅ Definition:\n",
    "A hybrid of **Ridge and Lasso**. Uses a linear combination of L1 and L2 penalties.\n",
    "\n",
    "#### 🧮 Loss Function:\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\left( \\alpha \\sum_{j=1}^{p} |\\beta_j| + (1 - \\alpha) \\sum_{j=1}^{p} \\beta_j^2 \\right)\n",
    "$$\n",
    "\n",
    "- $\\alpha$ = balance parameter between L1 and L2\n",
    "\n",
    "#### 🔍 Use Case:\n",
    "- Datasets with many features, especially correlated ones\n",
    "\n",
    "#### ✅ Advantages:\n",
    "- Combines strengths of Lasso and Ridge\n",
    "- Handles correlated variables better\n",
    "\n",
    "#### ❌ Limitations:\n",
    "- Requires tuning of two hyperparameters: $\\lambda$ and $\\alpha$\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Logistic Regression** *(for classification)*\n",
    "\n",
    "#### ✅ Definition:\n",
    "Used for **binary classification** problems (0/1, True/False). Despite the name, it's a classification algorithm.\n",
    "\n",
    "#### 🧮 Sigmoid Function:\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{1 + e^{-z}} \\quad \\text{where} \\quad z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "#### 🔍 Use Case:\n",
    "- Spam detection\n",
    "- Disease diagnosis (e.g., diabetic or not)\n",
    "\n",
    "#### ✅ Advantages:\n",
    "- Probabilistic interpretation\n",
    "- Efficient and interpretable\n",
    "\n",
    "#### ❌ Limitations:\n",
    "- Only handles binary output\n",
    "- Assumes linear boundary in log-odds space\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Stepwise Regression**\n",
    "\n",
    "#### ✅ Definition:\n",
    "A combination of forward selection and backward elimination to choose features step-by-step.\n",
    "\n",
    "- **Forward Selection**: Start with no variables, add one at a time.\n",
    "- **Backward Elimination**: Start with all variables, remove one at a time.\n",
    "\n",
    "#### 🔍 Use Case:\n",
    "- Feature selection in large datasets\n",
    "\n",
    "#### ✅ Advantages:\n",
    "- Systematic feature selection\n",
    "- Reduces model complexity\n",
    "\n",
    "#### ❌ Limitations:\n",
    "- Greedy approach—might miss best subset\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Quantile Regression**\n",
    "\n",
    "#### ✅ Definition:\n",
    "Instead of modeling the mean of the target variable, **quantile regression** models specific quantiles (like median, 25th percentile, etc.)\n",
    "\n",
    "#### 🧮 Loss Function:\n",
    "Minimizes asymmetric loss depending on quantile value $q$.\n",
    "\n",
    "#### 🔍 Use Case:\n",
    "- When we want to model medians or other quantiles instead of the mean\n",
    "- Used in finance for risk modeling\n",
    "\n",
    "#### ✅ Advantages:\n",
    "- Robust to outliers\n",
    "- Models conditional quantiles\n",
    "\n",
    "#### ❌ Limitations:\n",
    "- Interpretation can be complex\n",
    "- Less popular than OLS\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Bayesian Regression**\n",
    "\n",
    "#### ✅ Definition:\n",
    "Estimates the distribution of model parameters using **Bayesian inference**, instead of fixed point estimates.\n",
    "\n",
    "#### 🧮 Uses:\n",
    "- Priors and likelihood to obtain posterior distribution of weights\n",
    "\n",
    "#### 🔍 Use Case:\n",
    "- Uncertainty modeling\n",
    "- When data is scarce\n",
    "\n",
    "#### ✅ Advantages:\n",
    "- Captures uncertainty\n",
    "- Robust to small datasets\n",
    "\n",
    "#### ❌ Limitations:\n",
    "- Computationally expensive\n",
    "- Requires prior knowledge\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ **Core Assumptions in Linear Regression**\n",
    "\n",
    "1. **Linearity**: The relationship between input and output is linear.\n",
    "2. **Independence of errors**: Residuals are independent of each other.\n",
    "3. **Homoscedasticity**: Constant variance of errors across all values of independent variables.\n",
    "4. **Normality of residuals**: Errors are normally distributed (important for inference).\n",
    "5. **No multicollinearity**: Independent variables are not too highly correlated with each other.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### 🎯 **Model Evaluation Metrics**\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ **Mean Absolute Error (MAE)**  \n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$  \n",
    "- Measures the **average absolute difference** between actual and predicted values.  \n",
    "- **Interpretation**: Lower MAE = better fit  \n",
    "- **Sensitive to outliers**: No  \n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ **Mean Squared Error (MSE)**  \n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$  \n",
    "- Measures the **average of squared errors**.  \n",
    "- **Interpretation**: Penalizes larger errors more than MAE  \n",
    "- **Sensitive to outliers**: Yes  \n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ **Root Mean Squared Error (RMSE)**  \n",
    "$$\n",
    "RMSE = \\sqrt{MSE}\n",
    "$$  \n",
    "- **Square root of MSE**, making it interpretable in the same unit as the target.  \n",
    "- **Interpretation**: Lower RMSE = better model performance  \n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ **R-squared ($R^2$)**  \n",
    "$$\n",
    "R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}}\n",
    "$$  \n",
    "Where:  \n",
    "- $SS_{\\text{res}} = \\sum (y_i - \\hat{y}_i)^2$ (Residual Sum of Squares)  \n",
    "- $SS_{\\text{tot}} = \\sum (y_i - \\bar{y})^2$ (Total Sum of Squares)\n",
    "\n",
    "- Indicates the **proportion of variance in the target variable** explained by the model.  \n",
    "- $R^2$ ranges from **0 to 1**:\n",
    "  - 1 = perfect prediction\n",
    "  - 0 = model explains nothing\n",
    "- **Limitation**: Can **increase** with the addition of features, even if they're not useful.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ **Adjusted R-squared ($\\bar{R}^2$)**  \n",
    "$$\n",
    "\\bar{R}^2 = 1 - \\left(1 - R^2\\right) \\cdot \\frac{n - 1}{n - k - 1}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $n$ = number of observations  \n",
    "- $k$ = number of independent variables (predictors)  \n",
    "- $R^2$ = regular R-squared\n",
    "\n",
    "- **Improves R-squared** by adjusting for the number of predictors.  \n",
    "- **Penalizes** the model for adding features that do not improve predictive power.  \n",
    "- Can **decrease** if new variables don't add meaningful value.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔁 **Summary of Use Cases:**\n",
    "\n",
    "| Metric | Best When | Notes |\n",
    "|--------|-----------|-------|\n",
    "| MAE | Outliers need to be minimized | Easy to interpret |\n",
    "| MSE | Penalizing large errors is important | Sensitive to outliers |\n",
    "| RMSE | Interpretable and emphasizes large errors | Same units as target |\n",
    "| $R^2$ | Quick variance explanation | May mislead with many features |\n",
    "| Adjusted $R^2$ | Comparing models with different number of features | Better for model selection |\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### ⚖️ **Overfitting vs. Underfitting**\n",
    "\n",
    "| Concept       | Description                                                                 | Solution                                  |\n",
    "|---------------|-----------------------------------------------------------------------------|-------------------------------------------|\n",
    "| **Overfitting** | Model is too complex and fits noise in training data                         | Use regularization (Ridge, Lasso, Elastic Net), reduce complexity, cross-validation |\n",
    "| **Underfitting** | Model is too simple to capture underlying pattern                          | Use a more complex model or add features  |\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ **Model Selection Techniques**\n",
    "\n",
    "- **Train-Test Split**: Divide data into training and testing sets to evaluate performance.\n",
    "- **Cross-Validation (e.g., K-Fold)**: Systematically rotate through training/testing sets to ensure robustness.\n",
    "- **Grid Search**: Try combinations of hyperparameters and pick the best using validation score.\n",
    "- **Random Search**: Randomly sample hyperparameter combinations—faster and often effective.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 **Applications of Regression**\n",
    "\n",
    "- **Economics**: Predicting housing prices, GDP growth, inflation.\n",
    "- **Healthcare**: Estimating patient recovery time, disease likelihood.\n",
    "- **Marketing**: Forecasting sales, estimating ROI.\n",
    "- **Engineering**: Predicting machine wear, energy efficiency.\n",
    "- **Finance**: Modeling risk, pricing assets.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Summary**\n",
    "\n",
    "Regression is a fundamental tool in machine learning for modeling and predicting continuous outcomes. By understanding its types (simple, multiple, polynomial, regularized), assumptions, and evaluation techniques, you can build robust models that drive decisions across domains.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
