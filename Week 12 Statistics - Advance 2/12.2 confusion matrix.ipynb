{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://encord.com/glossary/confusion-matrix/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "A **confusion matrix** is a table used to evaluate the performance of a classification model in machine learning. It compares the actual labels of the data with the predictions made by the model. The matrix is particularly useful for understanding the types of errors the model is making.\n",
    "\n",
    "---\n",
    "\n",
    "## Structure of a Confusion Matrix\n",
    "\n",
    "For a **binary classification problem**, the confusion matrix is a 2x2 table with the following structure:\n",
    "\n",
    "|                       | **Predicted Positive** | **Predicted Negative** |\n",
    "|-----------------------|-------------------------|-------------------------|\n",
    "| **Actual Positive**   | True Positive (TP)      | False Negative (FN)     |\n",
    "| **Actual Negative**   | False Positive (FP)     | True Negative (TN)      |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Terms\n",
    "\n",
    "1. **True Positive (TP):** The model correctly predicts the positive class.\n",
    "2. **True Negative (TN):** The model correctly predicts the negative class.\n",
    "3. **False Positive (FP):** The model incorrectly predicts the positive class (Type I error).\n",
    "4. **False Negative (FN):** The model incorrectly predicts the negative class (Type II error).\n",
    "\n",
    "---\n",
    "\n",
    "## Metrics Derived from Confusion Matrix\n",
    "\n",
    "1. **Accuracy:** Measures the proportion of correct predictions.\n",
    "   $$\n",
    "   \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "   $$\n",
    "\n",
    "2. **Precision:** Measures the proportion of correctly predicted positive instances out of all predicted positive instances.\n",
    "   $$\n",
    "   \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "   $$\n",
    "\n",
    "3. **Recall (Sensitivity):** Measures the proportion of correctly predicted positive instances out of all actual positive instances.\n",
    "   $$\n",
    "   \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "   $$\n",
    "\n",
    "4. **F1-Score:** The harmonic mean of precision and recall.\n",
    "   $$\n",
    "   \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "5. **Specificity:** Measures the proportion of correctly predicted negative instances out of all actual negative instances.\n",
    "   $$\n",
    "   \\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "Suppose a model predicts whether an email is spam (positive) or not spam (negative). The confusion matrix might look like this:\n",
    "\n",
    "|                       | **Predicted Spam** | **Predicted Not Spam** |\n",
    "|-----------------------|---------------------|-------------------------|\n",
    "| **Actual Spam**       | 50 (TP)             | 10 (FN)                |\n",
    "| **Actual Not Spam**   | 5 (FP)              | 100 (TN)               |\n",
    "\n",
    "- **Accuracy:** \n",
    "  $$\n",
    "  \\frac{50 + 100}{50 + 10 + 5 + 100} = \\frac{150}{165} \\approx 90.9\\%\n",
    "  $$\n",
    "\n",
    "- **Precision:** \n",
    "  $$\n",
    "  \\frac{50}{50 + 5} = \\frac{50}{55} \\approx 90.9\\%\n",
    "  $$\n",
    "\n",
    "- **Recall:** \n",
    "  $$\n",
    "  \\frac{50}{50 + 10} = \\frac{50}{60} \\approx 83.3\\%\n",
    "  $$\n",
    "\n",
    "- **F1-Score:** \n",
    "  $$\n",
    "  2 \\times \\frac{0.909 \\times 0.833}{0.909 + 0.833} \\approx 86.9\\%\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- Evaluating classification models (binary and multi-class).\n",
    "- Identifying imbalanced class performance.\n",
    "- Debugging and improving model performance.\n",
    "\n",
    "For **multi-class classification**, the confusion matrix expands to an `n x n` table, where \\(n\\) is the number of classes. Each row represents the actual class, and each column represents the predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
